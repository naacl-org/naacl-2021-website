---
title: Tutorials
layout: single
excerpt: "NAACL 2019 Tutorials."
permalink: /program/tutorials/
sidebar: 
    nav: program
toc_label: "&nbsp;On this page"
toc: true
---
The following tutorials have been accepted for NAACL 2019 and will be held on Sunday, June 2nd, 2019. Exact timings will be posted later as part of the official schedule.


# Morning

## T1: Deep Adversarial Learning for NLP
<a href="https://www.cs.ucsb.edu/~william/">William Yang Wang</a>, <a href="http://sameersingh.org">Sameer Singh</a> and <a href="https://nlp.stanford.edu/~bdlijiwei/">Jiwei Li</a>

<div>
    <p>Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. Adversarial learning is also a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. </p>

    <p>In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming at providing some practical perspectives on the future of adversarial learning for solving real-world NLP problems.</p>
</div>

## T2: Deep Learning for Natural Language Inference
<a href="https://www.nyu.edu/projects/bowman/">Samuel Bowman</a> and <a href="http://xiaodanzhu.com">Xiaodan Zhu</a>

<div>
    <p>This tutorial discusses cutting-edge research on NLI, including recent advance on dataset development, cutting-edge deep learning models, and highlights from recent research on using NLI to understand capabilities and limits of deep learning models for language understanding and reasoning.</p>
</div>


## T3: Measuring and Modeling Language Change
<a href="https://www.cc.gatech.edu/~jeisenst/">Jacob Eisenstein</a>

<div>
    <p>This tutorial is designed to help researchers answer the following sorts of questions:
        <ul>
            <li> Are people happier on the weekend?</li>
            <li>What was 1861's word of the year?</li>
            <li>Are Democrats and Republicans more different than ever?</li>
            <li>When did gay stop meaning happy?</li>
            <li>Are gender stereotypes getting weaker, stronger, or just different?</li>
            <li>Who is a linguistic leader?</li>
            <li>How can we get internet users to be more polite and objective?</li>
        </ul>
    </p>

    <p>Such questions are fundamental to the social sciences and humanities, and scholars in these disciplines are increasingly turning to computational techniques for answers. Meanwhile, the ACL community is increasingly engaged with data that varies across time, and with the social insights that can be offered by analyzing temporal patterns and trends. The purpose of this tutorial is to facilitate this convergence in two main ways:
        <ol>
            <li> By synthesizing recent computational techniques for handling and modeling temporal data, such as dynamic word embeddings, the tutorial will provide a starting point for future computational research. It will also identify useful tools for social scientists and digital humanities scholars. </li>
            <li>The tutorial will provide an overview of techniques and datasets from the quantitative social sciences and the digital humanities, which are not well-known in the computational linguistics community. These techniques include vector autoregressive models, multiple comparisons corrections for hypothesis testing, and causal inference. Datasets include historical newspaper archives and corpora of contemporary political speech.</li>
        </ol>
    </p>
</div>


# Afternoon

## T4: Transfer Learning in Natural Language Processing
<a href="http://ruder.io">Sebastian Ruder</a>, <a href="https://www.linkedin.com/in/petersmatthew">Matthew Peters</a>, <a href="http://www.cs.cmu.edu/~sswayamd/index.html">Swabha Swayamdipta</a> and <a href="http://thomwolf.io">Thomas Wolf</a>

<div>
    <p>The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties.</p>

    <p>Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks.</p>

    <p>These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction.</p>

    <p>We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.</p>
</div>

## T5: Language Learning and Processing in People and Machines
<a href="http://www.aidanematzadeh.me">Aida Nematzadeh</a>, <a href="http://socsci.uci.edu/~rfutrell/">Richard Futrell</a> and <a href="http://www.mit.edu/~rplevy/index.html">Roger Levy</a>

<div>
    <p>The goal of this tutorial is to bring the fields of computational linguistics and computational cognitive science closer: we will introduce different stages of language acquisition and their parallel problems in NLP. As an example, one of the early challenges children face is mapping the meaning of word labels (such as "cat") to their referents (the furry animal in the living room). Word learning is similar to the word alignment problem in machine translation. We explain the current computational models of language acquisition, their limitations, and how the insights from these models can be incorporated into NLP applications. Moreover, we discuss how we can take advantage of the cognitive science of language in computational linguistics: for example, by designing cognitively-motivated evaluation tasks or buildings language-learning inductive biases into our models.</p>
</div>


## T6: Applications of Natural Language Processing in Clinical Research and Practice
<a href="https://sites.google.com/view/yanshanwang">Yanshan Wang</a>, <a href="https://www.linkedin.com/in/ahmadtafti">Ahmad Tafti</a>, <a href="https://www.linkedin.com/in/sunghwansohn">Sunghwan Sohn</a> and <a href="http://ruizhang.umn.edu">Rui Zhang</a>

<div>
    <p>Rapid growth in adoption of electronic health records (EHRs) has led to an unprecedented expansion in the availability of large longitudinal datasets. Large initiatives such as the Electronic Medical Records and Genomics (eMERGE) Network, the Patient-Centered Outcomes Research Network (PCORNet), and the Observational Health Data Science and Informatics (OHDSI) consortium, have been established and have reported successful applications of secondary use of EHRs in clinical research and practice. In these applications, natural language processing (NLP) technologies have played a crucial role as much of detailed patient information in EHRs is embedded in narrative clinical documents. Meanwhile, a number of clinical NLP systems, such as MedLEE, MetaMap/MetaMap Lite, cTAKES, and MedTagger have been developed and utilized to extract useful information from diverse types of clinical text, such as clinical notes, radiology reports, and pathology reports. Success stories in applying these tools have been reported widely.</p>

    <p>Despite the demonstrated success of NLP in the clinical domain, methodologies and tools developed for the clinical NLP are still under-known and under-utilized by students and experts in the general NLP domain, mainly due to the limited exposure to EHR data. Through this tutorial, we would like to introduce NLP methodologies and tools developed in the clinical domain, and showcase the real-world NLP applications in clinical research and practice at Mayo Clinic (the No. 1 national hospital ranked by the U.S. News \& World Report) and the University of Minnesota (the No. 41 best global universities ranked by the U.S. News \& World Report). We will review NLP techniques in solving clinical problems and facilitating clinical research, the state-of-the art clinical NLP tools, and share collaboration experience with clinicians, as well as publicly available EHR data and medical resources, and finally conclude the tutorial with vast opportunities and challenges of clinical NLP. The tutorial will provide an overview of clinical backgrounds, and does not presume knowledge in medicine or health care. The goal of this tutorial is to encourage NLP researchers in the general domain (as opposed to the specialized clinical domain) to contribute to this burgeoning area.</p>

    <p>In this tutorial, we will first present an overview of clinical NLP. We will then dive into two subareas of clinical NLP in clinical research, including big data infrastructure for large-scale clinical NLP and advances of NLP in clinical research, and two subareas in clinical practice, including clinical information extraction and patient cohort retrieval using EHRs. Around 70\% of the tutorial will review clinical problems, cutting-edge methodologies, and real-world clinical NLP tools while another 30\% introduce use cases at Mayo Clinic and the University of Minnesota. Finally, we will conclude the tutorial with challenges and opportunities in this rapidly developing domain.</p>
</div>  

